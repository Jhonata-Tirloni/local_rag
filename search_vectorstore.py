from transformers import pipeline
from langchain_core.documents import Document
from modules.local_vector_store import initialize_vector_store


modelPath: str = r"C:\Users\jhonata_tirloni\Documents\docchat-llm\src\models"
pipe = pipeline("text-generation",
                model=modelPath,
                device_map="auto")

vector_store = initialize_vector_store()


def rag_question(question_prompt: str, vector_results: list[Document]) -> None:
    """
    Receives a prompt and the result of a similarity search of a vector store.
    Then sends the data to a LLM model and returns the answered based on those two parameters.

    :param: question_prompt: str = The question to be answered by the LLM model
    :param: vector_results: Document = The result of a vectorstore similarity search

    :return: response: str = The response of the question, generated by the LLM, based on the
    content of the vector_results parameter.
    """
    info: str = vector_results[0].page_content

    message: list = [
        {
            "role": "system",
            "content": "You will be shown the user's question, and the relevant information from a document of that topic. Answer the user's question using this information and also give some tips based on your knowledge (as long as it represents just 5% of the whole context of the answer)",
        },
        {
            "role": "user",
            "content": f"Question: {question_prompt}. \n Information: {info}",
        },
    ]

    output = pipe(message, max_new_tokens=950)
    response: str = output[0]["generated_text"][2]["content"]

    return print(response)


prompt: str = input("Qual sua pergunta?: ")

results: list[Document] = vector_store.similarity_search(query=prompt, k=1)

rag_question(question_prompt=prompt, vector_results=results)
